{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium - Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przedmiot: SISI\n",
    "\n",
    "Autor: Rafał Stachura\n",
    "\n",
    "kontakt: rafstach.97@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Celem ćwiczenia jest zapoznanie się z technologią Object Detection. W ćwiczeniu zostanie wykorzystany gotowy model detekcji pretrenowany na zbiorze COCO. Rozpoznawanie obiektów na obrazie będzie realizowane z wykorzystaniem TensorFlow Lite, a do przetwarzania obrazów zostanie użyta biblioteka OpenCV. Ćwiczenie zostało podzielone na dwie części. W pierszej zostanie detekcja na statycznych obrazach, natomiast w drugiej części zostanie zrealizowane rozpoznawanie obiektów na żywo z wykorzystaniem kamery internetowej. Z powodu konieczności dostępu do kamery zalecana jest realizacja laboratorium na urządzeniu lokalnym.\n",
    "\n",
    "#### Zbiór danych COCO\n",
    "Informacje o zbiorze danych: http://cocodataset.org/#home\n",
    "\n",
    "#### Zbiór gotowych modeli detekcji dla Tensorflow\n",
    "Źródło: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
    "\n",
    "#### Tensorflow Object Detection API\n",
    "Informacje na temat dostepnego API i jego konfiguracji: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny i omówienie narzędzi\n",
    "\n",
    "### Object Detection\n",
    "Detekcja obiektów na obrazie należy do zagadnień związanych z Computer Vision. Jest to technologia powiązana z przetwarzaniem obrazów, która pozwala rozpoznać na wejściowym obrazie obiekty należące do określonej klasy pochodzącej ze znanego zbioru. Wejściem dla modeli Object Detection mogą być nie tylko obrazy, ale również strumienie wideo. Algorytm po przetworzeniu danej ramki zwraca informację, które z obiektów ze zdefiniowanego zbioru zostały rozpoznane i jednocześńnie podaje ich pozycję np. w postaci listy ramek. Dosepna jest również informacja o pewności z jaką rozpoznanym obiektom została przypisana dana klasa.\n",
    "\n",
    "#### Przykłady zastosowań:\n",
    "- Adnotowanie obrazów\n",
    "- Detekcja i rozpoznawanie twarzy\n",
    "- Aktywne rozpoznawanie obiektów\n",
    "\n",
    "### TensoFlow Lite\n",
    "Jest to framework dla uczenia maszynowego przeznaczony dla urządzeń o ogranicoznych zasobach sprzętowych. Narzędzie dostarcza wszystkich potrzebnych narzędzi, potrzebnych do konwersji oraz uruchamiania modeli TensorFlow na urządzeniach mobilnych, sprzecie klasy embeded oraz szeroko pojętych urządzeniach IoT. Jest często wykorzystywany w Edge Computing.  \n",
    "Dowiedz się więcej: https://www.tensorflow.org/lite\n",
    "\n",
    "### OpenCV\n",
    "Wieloplatformowa biblioteka do przetwarzani obrazów posiadająca interfejsy programistyczne w wielu językach np. Python, Java. Narzędzie gładzie ancisk na przetwarzanie obrazów  wczasie rzeczywistym.\n",
    "Dowiedz się więcej: https://opencv.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ogólna zasada działania frameworków Object Detection\n",
    "1. Model głębokiego uczenia maszynowego lub stosowny algorytm dokonuje podziału obrazu za pomocą wygenerowanego, zazwyczaj bardzo licznego, zbioru ramek ograniczających elementy na obrazie. Fragment algorytmu odpowiedzialny za generowanie zbioru ograniczających ramek nazywany jest komponentem lokalizacyjnym.\n",
    "<img src=\"doc-img/divided_picture.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "2. Dla każdej wydzielonej ramki nastepuje ekstrakcja cech. Komponnt klasyfikacji dla każdego fragmentu dokonuje ewaluacji czy wewnątrz określonej ramki znajduje się obiekt, a jeżeli, tak, to jaki.\n",
    "<img src=\"doc-img/feature_extraction_picture.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "3. W ostatnim kroku nachodzące ramki są redukowane do pojedynczych komponentów, dla których został przewidziany określony obiekt na ilustracji.\n",
    "<img src=\"doc-img/objct_detected_picture.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "Źródło ilustracji: https://www.analyticsvidhya.com/blog/2020/04/build-your-own-object-detection-model-using-tensorflow-api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sieć neuronowa MobileNet-SSD\n",
    "\n",
    "#### SSD - Single Shot MultiBox Detector\n",
    "Architektura modelu SSD została omówiona w artykule SSD: Single Shot MultiBox Detector(https://arxiv.org/abs/1512.02325)z 2016 roku. Nazwa modelu zawiera następujce informacje:\n",
    "- Single Shot - zadanie lokalizacji obiektów na zdjęciu i ich klasyfikacja jest realizowana w pojedynczym przebiegu sieci neuronowej\n",
    "- MultiBox - odnosi się do techniki przewidywania ramek rozwijanej przez Szegedy.\n",
    "- Detector - sieć neuronowo dokonuje detekcji obiektów i je kalsyfikuje\n",
    "\n",
    "#### Architektura sieci\n",
    "<img src=\"doc-img/mobile_net_architecture.png\" width=\"900\" height=\"400\">\n",
    "Źródło ilustarcji: https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab\n",
    "\n",
    "Architektura sieci SSD jest pojedynczą siecią konwolucyjną zdolną do przewidywania, położenia ramek ograniczających i klasyfikowania ich w czasie pojedynczego przebiegu. Sieć SSD skłąda się z bazowej architektury (MobileNet) oraz kilku warstw konwolucyjnych nastepująccyh po niej.\n",
    "\n",
    "Sieć operuje na mapie cech, by wykryć ramki ograniczające. Sieć nie przewiduje kształtu ramki, a jedynie jej położenie na ilustracji. Każda z K wygenerowanych ramek dla każdej z mapy cech posiada predefiniowany kształt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja środowiska\n",
    "\n",
    "Lokalna konfiguracja środowiska obejmuje instalację niezbędnych pakietów oraz ich zaimportowanie. W drugej części ćwiczenia potrzbna będzie kamera internetowa - można wykorzystać tą dostępną w laptopie. W przypadku problemów z dostępem do urządzenia należy zwrócić uwage na nadanie stosownych uprawnień."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import niezbędnych pakietów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pathlib\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konfiguracja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład użycia\n",
    "Przykład obejmuje załadnowanie przykładowych obrazów oraz identyfikację przedstawionych na nich obiektów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie modelu\n",
    "\n",
    "Należy pobrać wytrenowany model dla TensorFlow Lite dostepny pod adresem: https://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip, a następnie wypakować go do katalogu model. Po rozpakowaniu w katalogu pojawią się dwa pliki: \n",
    "- detect.tflit - model sieci neuronowej coco_ssd_mobilenet_v1_1.0_quant wytrenowanej i poddanenej kwantyzacji. Jest to model należący do klasy MobileNets, cyli modeli obejmujących konwolucyjne sieci neuronowe dla urzadzeń mobilnych.\n",
    "- labelmap.txt - plik tekstowy zawierajacy zorganizowane w pojedynczej kolumnie etykiety klas obiektów rozpoznawanych przez model(uwaga, pierwsza linia zawiera `???`, które są pomijane w dalszej części kodu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wczytanie modelu i alokacja tensorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załadowanie wytrenowanego modelu TFLite\n",
    "path = 'model/detect.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=path)\n",
    "\n",
    "# Alokacja tensorów\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wyświetlenie informacji o tensorach wejściowych i wyjściowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensory wejściowe:\n",
      " [{'name': 'normalized_input_image_tensor', 'index': 175, 'shape': array([  1, 300, 300,   3]), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 128)}]\n",
      "Tensory wyjściowe:\n",
      " [{'name': 'TFLite_Detection_PostProcess', 'index': 167, 'shape': array([ 1, 10,  4]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 168, 'shape': array([ 1, 10]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 169, 'shape': array([ 1, 10]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 170, 'shape': array([1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n"
     ]
    }
   ],
   "source": [
    "# Pobierz informacje o wejściowych i wyjściowych tensorach\n",
    "\n",
    "# Odkomentować w wersji studenckiej\n",
    "# input_details = ?\n",
    "# output_details = ?\n",
    "\n",
    "#Oczekiwane rozwiązanie\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Tensory wejściowe:\\n\",input_details)\n",
    "print(\"Tensory wyjściowe:\\n\",output_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretaja informacji o tensorach\n",
    "Na podstawie wypisanych informacji określ, jaki jest oczekiwany przez model wejściowy format obrazu oraz w jakiej postaci są zwracane wynki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wejście modelu:\n",
    " - szerokość wejściowa = ? //300\n",
    " - wysokość wejściowa = ? // 300\n",
    " - format piksela = ? //rgb\n",
    "\n",
    "Wyjście modelu:\n",
    " - format wyjściowy = ? // ramki  określające położenie obiektu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wczytanie etykiet obiektów rozpoznawanych przez model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczyt etykiet\n",
    "path_to_labels = 'model/labelmap.txt'\n",
    "\n",
    "with open(path_to_labels, 'r') as file:\n",
    "    labels = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Usunięcie nagłówka z pliku    \n",
    "if labels[0] == '???':\n",
    "    del(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zapoznanie z listą rozpoznawanych obiektów\n",
    "Wypisz kilka przykładowych etykiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
       "       'train', 'truck', 'boat', 'traffic light'], dtype='<U14')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ?\n",
    "# Usunąć w wersji studenckiej\n",
    "np.array(labels)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wczytanie obrazu wejściowego do detekcji obrazu\n",
    "Po wczytaniu obrazu należy dostosować jego rozmiar do formatu akceptowanego przez przez model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie obrazu\n",
    "image_path = 'images/computer.png'\n",
    "img = cv2.imread(r\"{}\".format(pathlib.Path(image_path).resolve()))\n",
    "    \n",
    "# Dopasuj kształt obrazu do formatu akceptowanego przez model\n",
    "# Odkomentować w wersji studenckiej\n",
    "# img_reshaped = ?\n",
    "img_reshaped = cv2.resize(img, (300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uruchomienie modelu Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.14809573  0.30314285  0.57909644  0.77701765]\n",
      "  [ 0.13584289 -0.00234103  0.7962525   0.27592713]\n",
      "  [ 0.6496609   0.27141908  0.7684035   0.907462  ]\n",
      "  [ 0.6544368   0.38669217  0.74279773  0.8803977 ]\n",
      "  [ 0.51823425  0.0134017   0.7690197   0.2716472 ]\n",
      "  [ 0.16821927  0.19377878  0.76761925  0.8724085 ]\n",
      "  [ 0.18516573  0.43858016  0.6161665   0.79129815]\n",
      "  [ 0.15562253  0.30035827  0.42523187  0.75228786]\n",
      "  [ 0.6298785   0.35913348  0.7089846   0.84509706]\n",
      "  [ 0.7272335   0.18582633  0.95424175  0.9547049 ]]]\n",
      "[[0.78515625 0.48828125 0.4609375  0.40234375 0.35546875 0.28125\n",
      "  0.25       0.2421875  0.234375   0.234375  ]]\n",
      "[[71. 76. 75. 75. 71. 71. 71. 71. 75. 75.]]\n"
     ]
    }
   ],
   "source": [
    "# Ustawienie tensorów\n",
    "interpreter.set_tensor(input_details[0]['index'], [img_reshaped])\n",
    "\n",
    "# Uruchomienie interpretera\n",
    "interpreter.invoke()\n",
    "\n",
    "# Odczyt rezultatów\n",
    "rectangles = interpreter.get_tensor(output_details[0]['index'])\n",
    "classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "print(rectangles)\n",
    "print(scores)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zapoznanie z wynikami\n",
    "\n",
    "Jaka jest postać wyników działania modelu detekcji elementów na obrazie? W jaki sposób można wykorzystać otrzymane informacje do wizualizacji efektów działania modelu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funkcja pomocnicza zazaczająca prostokątny obszar na obrazie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(image, box, recognized_class, result_score):\n",
    "    img_height = image.shape[0]\n",
    "    img_width = image.shape[1]\n",
    "    y_min = int(max(1, (box[0] * img_height)))\n",
    "    x_min = int(max(1, (box[1] * img_width)))\n",
    "    y_max = int(min(img_height, (box[2] * img_height)))\n",
    "    x_max = int(min(img_width, (box[3] * img_width)))\n",
    "    \n",
    "    # Zaznaczenie prostokątnego obszaru an obrazie\n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 1)\n",
    "    \n",
    "    # Dodanie etykiety obiektu do obrazu\n",
    "    object_name = labels[int(recognized_class)]\n",
    "    label = '%s: %d%%' % (object_name, int(result_score*100))\n",
    "    labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) \n",
    "    label_ymin = max(y_min, labelSize[1] + 10)\n",
    "    cv2.rectangle(image, (x_min, label_ymin-labelSize[1]-10), (x_min+labelSize[0], label_ymin+baseLine-10), (0, 255, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, (x_min, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detekcja obiektów na przykłądowych obrazach\n",
    "Przeprowadzana jest detekcja obeiktów na obrazach dla wszystkich obrazów z katalogu images. Naciśnij klawisz, aby zmienić obraz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wciśnij enter aby kontynuować...\n"
     ]
    }
   ],
   "source": [
    "images_dir_path = 'images'\n",
    "min_accuracy = 0.50\n",
    "images_after_object_detection = []\n",
    "\n",
    "for file in pathlib.Path(images_dir_path).iterdir():\n",
    "    # Odczyt obrazu i dopasowanie rozmiatu\n",
    "    img = cv2.imread(r\"{}\".format(file.resolve()))\n",
    "    img_reshaped = cv2.resize(img, (300, 300))\n",
    "\n",
    "    # Uruchomienie modelu\n",
    "    interpreter.set_tensor(input_details[0]['index'], [img_reshaped])\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Pobranie rezultatów\n",
    "    rectangles = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n",
    "    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n",
    "    \n",
    "    # Dodanie zaznaczenia dla wszystkich obiektów o accuracy > 0.5\n",
    "    for index, score in enumerate(scores):\n",
    "        if score > min_accuracy:\n",
    "            draw_rectangle(img_reshaped, rectangles[index], classes[index], scores[index])\n",
    "    \n",
    "    # Zmiana formatu piksela\n",
    "    img_reshaped = cv2.cvtColor(img_reshaped, cv2.COLOR_BGR2RGB)    \n",
    "    images_after_object_detection.append(img_reshaped)\n",
    "\n",
    "\n",
    "\n",
    "for img in images_after_object_detection:\n",
    "    imshow(img, aspect ='auto')\n",
    "    show()\n",
    "    clear_output(wait=True)\n",
    "    input(\"Wciśnij enter aby kontynuować...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Odczyt obrazu z kamery internetowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zasób wideo został zwolniony\n"
     ]
    }
   ],
   "source": [
    "# Ustalenie wejściowego obrazu z kamery internetowej (opcjonalnie można jako argument podać ścieżkę do filmu np. mp4)\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# Obraz jest wyświetlany w notatniku na żywo do momentu przerwania\n",
    "try:\n",
    "    while(True):\n",
    "        # Pobranie pojedynczej ramki\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        if not ret:\n",
    "            video.release()\n",
    "            print(\"Zasób wideo został zwolniony\")\n",
    "            break\n",
    "            \n",
    "        # Konwersja formatu obrazu\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axis('off')\n",
    "        title(\"Obraz wideo\")\n",
    "        \n",
    "        # Wyświetlenie pojedynczej ramki\n",
    "        imshow(frame, aspect ='auto')\n",
    "        show()\n",
    "        clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    # Zwolnienie zasobu po zakończeniu odczytu obrazu\n",
    "    video.release()\n",
    "    print(\"Zasób wideo został zwolniony\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Detection - na żywo\n",
    "\n",
    "Na podstawie przedstawionych fragmentów należy przygotować kod, który będzie realizował detekcję obiektów na obrazie pobieranym z kamery na żywo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funkcja pomocnicza realizująca detekcję obiektów na pojedynczej ramce\n",
    "Należy zaimplementować funkcję"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odkomentować w wersji studenckiej\n",
    "# def object_detection_on_single_frame(frame):\n",
    "#    ?\n",
    "\n",
    "def object_detection_on_single_frame(frame):\n",
    "    img_reshaped = cv2.resize(frame, (300, 300))\n",
    "\n",
    "    # Uruchomienie modelu\n",
    "    interpreter.set_tensor(input_details[0]['index'], [img_reshaped])\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Pobranie rezultatów\n",
    "    rectangles = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n",
    "    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n",
    "    \n",
    "    # Dodanie zaznaczenia dla wszystkich obiektów o accuracy > 0.5\n",
    "    for index, score in enumerate(scores):\n",
    "        if score > min_accuracy:\n",
    "            draw_rectangle(img_reshaped, rectangles[index], classes[index], scores[index])\n",
    "            \n",
    "    return img_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integracja algorytmu Object Detection z obrazem transmitowanym na żywo\n",
    "Należy połączyć gragment kodu odczytujący obraz z kamery z funkcja dokonującą detekcji okbiektów na obrazie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zasób wideo został zwolniony\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ustalenie wejściowego obrazu z kamery internetowej (opcjonalnie można jako argument podać ścieżkę do filmu np. mp4)\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# Obraz jest wyświetlany w notatniku na żywo do momentu przerwania\n",
    "try:\n",
    "    while(True):\n",
    "        # Pobranie pojedynczej ramki\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        if not ret:\n",
    "            video.release()\n",
    "            print(\"Zasób wideo został zwolniony\")\n",
    "            break\n",
    "            \n",
    "        # Konwersja formatu obrazu z używanego przez OpenCV na standardową formę plaety\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axis('off')\n",
    "        title(\"Obraz wideo\")\n",
    "        \n",
    "        # Przetworzenie ramki algorytmem detekcji obrazu i zaznaczenie znalezionych obiektów\n",
    "        processed_img = object_detection_on_single_frame(frame)\n",
    "        \n",
    "        # Wyświetlenie pojedynczej ramki po zastosowaniu Object Detection\n",
    "        imshow(processed_img, aspect ='auto')\n",
    "        show()\n",
    "        clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    # Zwolnienie zasobu po zakończeniu odczytu obrazu\n",
    "    video.release()\n",
    "    print(\"Zasób wideo został zwolniony\") \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Źródła:\n",
    "- https://www.analyticsvidhya.com/blog/2020/04/build-your-own-object-detection-model-using-tensorflow-api/\n",
    "- https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
